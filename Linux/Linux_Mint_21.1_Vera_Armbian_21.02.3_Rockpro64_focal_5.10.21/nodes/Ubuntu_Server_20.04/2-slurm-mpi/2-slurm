#!/bin/bash
#Este script Ã© executado atraves de um script primario
#second opt Set up only torque in the nodes
w=$(tput sgr0) 
r=$(tput setaf 1)
g=$(tput setaf 2) 
y=$(tput setaf 3)
p=$(tput setaf 5)  

USAGE="$0 -n <nodename> -c <clustername> -u <username>"

if [ "$#" -lt "4" ] 
then 
    echo -e $USAGE;
exit 1    
else 
    var1=$2;
    var2=$4;
fi

while getopts n:c:u: option
do
case "${option}"
in
n) node_name=${OPTARG};;
c) cluster_name=${OPTARG};;
u) user_name=${OPTARG};;
esac
done

if [ "$node_name" = "" ]; then
"Error, missing arguments
Try: $USAGE"
exit 1
fi
if [ "$cluster_name" = "" ]; then
"Error, missing arguments
Try: $USAGE"
exit 1
fi
if [ "$user_name" = "" ]; then
"Error, missing arguments
Try: $USAGE"
exit 1
fi

echo -e "${g}"
echo	"-----------------------------------------------------------------------------------"
echo    "------------------------------ Configuring SLURM ----------------------------------"
echo    "-----------------------------------------------------------------------------------"
echo "Node name to connect:${w}"
echo "$node_name"
echo "${g}Cluster name:${w}"
echo "$cluster_name"
echo "${g}User name:${w}"
echo "$user_name"
echo "${g}----------------------------------------------------------------------------------"

echo "${g}"
echo "Installing required packages${w}
"
ssh $node_name apt-fast update
ssh $node_name 'apt-fast install -y libmunge-dev libmunge2 munge'
scp /etc/munge/munge.key root@$node_name:/etc/munge/
ssh $node_name "chown munge:munge /etc/munge/munge.key"
ssh $node_name "chmod 400 /etc/munge/munge.key"
ssh $node_name "systemctl enable munge"
ssh $node_name "systemctl restart munge"
echo "${g}"
echo "-----------------------------"
echo "Testing MUNGE...${w}"
ssh $node_name "munge -n | unmunge | grep STATUS"
ssh $node_name "munge -n | ssh master unmunge | grep STATUS"
echo "${g}-----------------------------"
echo "${w}"
#Install Slurm

slurm_url=https://download.schedmd.com/slurm/slurm-22.05.0-0rc1.tar.bz2
slurm_tgz=slurm-22.05.0-0rc1.tar.bz2
slurm_dir=/node_temp/slurm-22.05.0-0rc1
slurm_deb=slurm-22.05.0-0rc1_1.0_amd64.deb
slurm_v=slurm-22.05.0-0rc1
ssh $node_name "rm -rf /storage /node_temp/"
ssh $node_name "apt-fast update -qq"
#ssh $node_name "apt-fast install git gcc make ruby ruby-dev libpam0g-dev libmariadbclient-dev libmariadb-dev libmysqlclient-dev -y"
ssh $node_name "apt-fast install git gcc make ruby ruby-dev libpam0g-dev libmysqlclient-dev -y"
ssh $node_name "gem install fpm"
ssh $node_name "mkdir -p /storage"
ssh $node_name "mkdir -p /node_temp/"
ssh $node_name "cd /node_temp/ && wget $slurm_url"
ssh $node_name "cd /node_temp/ && tar xf $slurm_tgz"
ssh $node_name "cd $slurm_dir && ./configure --prefix=/tmp/slurm-build --sysconfdir=/etc/slurm --enable-pam --with-pam_dir=/lib/x86_64-linux-gnu/security/ --without-shared-libslurm"
ssh $node_name "cd $slurm_dir && make -j $threads"
ssh $node_name "cd $slurm_dir && make contrib -j $threads"
ssh $node_name "cd $slurm_dir && make install -j $threads"
ssh $node_name "cd /node_temp/ && fpm -s dir -t deb -v 1.0 -n $slurm_v --prefix=/usr -C /tmp/slurm-build ."
ssh $node_name "cd /node_temp/ && dpkg -i --force-all $slurm_deb"
ssh $node_name "rm -rf /node_temp/"
ssh $node_name "mkdir -p /etc/slurm"

#update slurm.conf without GPU
cat /etc/slurm/slurm.conf | grep "$node_name" &> /dev/null
if [ $? == 0 ]; then
echo "$node_name ${g}is already inserted in the master's slurm.conf file${w}"
else
    scp 2-slurm-deps/node.slurm.conf.no-gpu root@$node_name:/root/
    ssh $node_name "chmod +x node.slurm.conf.no-gpu"
    scp /etc/slurm/slurm.conf root@$node_name:/etc/slurm/slurm.conf
    ssh $node_name "/root/node.slurm.conf.no-gpu >> /etc/slurm/slurm.conf"
    scp root@$node_name:/etc/slurm/slurm.conf /etc/slurm/slurm.conf
rm /root/node.slurm.conf.no-gpu
 fi

#root@node
scp 2-slurm-deps/cgroup.conf root@$node_name:/etc/slurm/cgroup.conf
ssh $node_name "useradd slurm" 
ssh $node_name "mkdir -p /var/spool/slurm/d"
scp 2-slurm-deps/slurmd.service root@$node_name:/etc/systemd/system/

ssh $node_name "systemctl enable slurmd"
ssh $node_name "systemctl restart slurmd"

#root@master
service slurmctld restart

scontrol update state=idle nodename=$node_name
#scontrol update state=down nodename=$node_name reason=config

#Test Slurm
echo "${g}"
sinfo -l -N | grep NODELIST && sinfo -l -N | grep "$node_name"

echo "${w}"
ssh $node_name "sed -i 's|GRUB_CMDLINE_LINUX=\"\"|GRUB_CMDLINE_LINUX=\"cgroup_enable=memory swapaccount=1\"|g' /etc/default/grub && update-grub"

ssh $node_name -q "reboot"

echo "${g}"
echo "----------------------------------------------------------"
echo "${p}$node_name ${g}was rebooted, waiting until it is online."
echo "----------------------------------------------------------"
sleep 5 && ssh $node_name -q "exit" 
while test $? -gt 0
do
   sleep 10
   echo "Trying ssh to${y} $node_name${g}..."
   ssh $node_name -q "exit"
done

ssh $node_name "systemctl restart slurmd"
scontrol update state=idle nodename=$node_name
#scontrol update state=down nodename=$node_name reason=config
sinfo -l -N | grep NODELIST && sinfo -l -N | grep "$node_name"

echo "${g}"
sinfo

echo "${w}"
echo "${w}"
echo "-----------------------------------------------------"
echo "Testing a SLURM job, press${y} CTRL+C ${w}to ignore this test"
echo "-----------------------------------------------------"
scp -r /etc/slurm/slurm.conf root@$node_name:/etc/slurm/
scontrol update state=idle nodename=$node_name
#scontrol update state=down nodename=$node_name reason=config
su -c "srun -w $node_name hostname" $user_name 

echo "${g}"
echo ""
echo -e "setf 2" | tput -S
echo	"----------------------------------------------------------------------------------"
echo    "-------------------------- Slurm configured successfully -------------------------"
echo    "----------------------------------------------------------------------------------"	


